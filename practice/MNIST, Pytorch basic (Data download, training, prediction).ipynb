{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled99.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5jbeLHsDe8t"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader \n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFFxxthdLAJc"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "class FeedForwardNet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dense_layers = nn.Sequential(\n",
        "            nn.Linear(28*28, 256), # keras Dense\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256,10) # number of class at MNIST is 10\\\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        flattened_data = self.flatten(input_data)\n",
        "        logits = self.dense_layers(flattened_data)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4Es9zpsHeu8"
      },
      "source": [
        "# 1 - download dataset\n",
        "\n",
        "def download_mnist_datasets():\n",
        "    train_data = datasets.MNIST(\n",
        "        root=\"data\",   # root directory\n",
        "        download=True,\n",
        "        train=True,   # train set\n",
        "        transform=ToTensor()    # What is ToTensor\n",
        "    )\n",
        "    validation_data = datasets.MNIST(\n",
        "        root=\"data\",   # root directory\n",
        "        download=True,\n",
        "        train=False,   # validation set\n",
        "        transform=ToTensor()   \n",
        "    )\n",
        "    return train_data, validation_data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyPluKFcKTvk"
      },
      "source": [
        "# download MNIST dataset\n",
        "\n",
        "train_data, _ = download_mnist_datasets()\n",
        "print('MNIST dataset downloaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqu34mZ7fhwQ"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcKPqJ4BaMd4"
      },
      "source": [
        "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for inputs, targets in data_loader:\n",
        "        inputs,targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # calculate loss\n",
        "        predictions = model(inputs)\n",
        "        loss = loss_fn(predictions,targets)\n",
        "\n",
        "        # backpropagate loss and update weights\n",
        "        optimiser.zero_grad() # make gradient zero\n",
        "        loss.backward()\n",
        "        optimiser.step() \n",
        "\n",
        "    print(f\"Loss : {loss.item()}\")\n",
        "\n",
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Training is done.\")\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jptx_3bpKxKv",
        "outputId": "b69672a8-0916-4728-e5fc-e7d103ad2262"
      },
      "source": [
        "# create a data loader for the train set\n",
        "# this case, we don't use val data\n",
        "\n",
        "train_data_loader = DataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# build model\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "feed_forward_net = FeedForwardNet().to(device)\n",
        "\n",
        "# instantiate loss function + optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(feed_forward_net.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "\n",
        "train(feed_forward_net, train_data_loader,loss_fn, optimiser, device, EPOCHS)\n",
        "\n",
        "torch.save(feed_forward_net.state_dict(), \"feedforwardnet.pth\")\n",
        "\n",
        "print(\"Model trained and stored at feedforwardnet.pth\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "Epoch 1\n",
            "Loss : 1.5099091529846191\n",
            "---------------------------\n",
            "Epoch 2\n",
            "Loss : 1.4991612434387207\n",
            "---------------------------\n",
            "Epoch 3\n",
            "Loss : 1.496337890625\n",
            "---------------------------\n",
            "Epoch 4\n",
            "Loss : 1.4842170476913452\n",
            "---------------------------\n",
            "Epoch 5\n",
            "Loss : 1.4768468141555786\n",
            "---------------------------\n",
            "Epoch 6\n",
            "Loss : 1.4741939306259155\n",
            "---------------------------\n",
            "Epoch 7\n",
            "Loss : 1.473215103149414\n",
            "---------------------------\n",
            "Epoch 8\n",
            "Loss : 1.4733935594558716\n",
            "---------------------------\n",
            "Epoch 9\n",
            "Loss : 1.4727405309677124\n",
            "---------------------------\n",
            "Epoch 10\n",
            "Loss : 1.4731807708740234\n",
            "---------------------------\n",
            "Training is done.\n",
            "Model trained and stored at feedforwardnet.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I_-nU7ilQI0"
      },
      "source": [
        "# Model loading and prediction\n",
        "\n",
        "import torch\n",
        "# from train import FeedForwardNet, download_mnist_datasets "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHtEBtailhxm",
        "outputId": "b09816e5-51b0-48c4-cd29-e89fbe3ba6c5"
      },
      "source": [
        "class_mapping = [\n",
        "                 \"0\",\n",
        "                 \"1\",\n",
        "                 \"2\",\n",
        "                 \"3\",\n",
        "                 \"4\",\n",
        "                 \"5\",\n",
        "                 \"6\",\n",
        "                 \"7\",\n",
        "                 \"8\",\n",
        "                 \"9\"\n",
        "]\n",
        "\n",
        "def predict(model, input, target, class_mapping): #class_mapping이 꼭 필요한가?\n",
        "    model.eval()\n",
        "    #mode.train()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input)\n",
        "        # Tesnor (1, 10) -> one sample, 10 classes\n",
        "        predicted_index = predictions[0].argmax(0)\n",
        "        predicted = class_mapping[predicted_index]\n",
        "        expected = class_mapping[target]\n",
        "    return predicted, expected\n",
        "\n",
        "# load back the model\n",
        "\n",
        "feed_forward_net = FeedForwardNet()\n",
        "state_dict = torch.load(\"feedforwardnet.pth\")\n",
        "feed_forward_net.load_state_dict(state_dict)\n",
        "\n",
        "# load MNIST validation dataset\n",
        "_, validation_data = download_mnist_datasets()\n",
        "\n",
        "# get a sample from the validation dataset for inference\n",
        "input, target = validation_data[0][0], validation_data[0][1]\n",
        "\n",
        "# make an inference\n",
        "predicted, expected = predict(feed_forward_net, input, target, class_mapping)\n",
        "print(f\"Predicted: '{predicted}', expected : '{expected}'\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted: '7', expected : '7'\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}