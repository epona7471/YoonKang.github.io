{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled100.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzdwVlH_MmGV",
        "outputId": "d188e09a-7d02-4c4e-92fb-7915ec8b83c3"
      },
      "source": [
        "!wget https://goo.gl/8hY5ER"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-22 07:51:15--  https://goo.gl/8hY5ER\n",
            "Resolving goo.gl (goo.gl)... 108.177.127.113, 108.177.127.102, 108.177.127.100, ...\n",
            "Connecting to goo.gl (goo.gl)|108.177.127.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz [following]\n",
            "--2021-07-22 07:51:15--  https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6023741708 (5.6G) [application/octet-stream]\n",
            "Saving to: ‘8hY5ER’\n",
            "\n",
            "8hY5ER              100%[===================>]   5.61G  49.0MB/s    in 63s     \n",
            "\n",
            "2021-07-22 07:52:19 (91.5 MB/s) - ‘8hY5ER’ saved [6023741708/6023741708]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLly8dwrYWfT"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPFzVs92NQsX"
      },
      "source": [
        "import shutil\n",
        "shutil.unpack_archive(\"8hY5ER.tar\", \"./urban\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5TpleDYBxWn"
      },
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "\n",
        "class UrbanSoundDataset(Dataset):\n",
        "\n",
        "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate): # path to csv, path to audio folder\n",
        "        self.annotations = pd.read_csv(annotations_file)\n",
        "        self.audio_dir = audio_dir\n",
        "        self.transformation = transformation\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_sample_path = self._get_audio_sample_path(index)\n",
        "        label = self._get_audio_sample_label(index)\n",
        "        signal, sr = torchaudio.load(audio_sample_path)\n",
        "        signal = self._resample_if_necessary(signal, sr)\n",
        "        signal = self._mix_down_if_necessary(signal) # stereo to mono\n",
        "        signal = self.transformation(signal)\n",
        "        return signal, label\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        if signal.shape[0] > 1: # (2, 1000)\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "\n",
        "    def _get_audio_sample_path(self, index):\n",
        "        fold = f\"fold{self.annotations.iloc[index, 5]}\" # 5th column 'Fold'\n",
        "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0] )\n",
        "        return path\n",
        "    \n",
        "    def _get_audio_sample_label(self, index):\n",
        "        return self.annotations.iloc[index, 6]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi6XbfCdaAV3",
        "outputId": "913f4ea5-9ad3-4497-8461-5fbd0a34ca46"
      },
      "source": [
        "ANNOTATIONS_FILE = '/content/urban/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
        "AUDIO_DIR = '/content/urban/UrbanSound8K/audio'\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "usd = UrbanSoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, mel_spectrogram,\n",
        "                        SAMPLE_RATE)\n",
        "\n",
        "print(f\"There are {len(usd)} samples in the dataset.\")\n",
        "signal, label = usd[0]\n",
        "\n",
        "a=1"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 8732 samples in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFmx4eTKl5nI",
        "outputId": "8e896544-58e4-429f-9ac0-d7fe551be653"
      },
      "source": [
        "signal.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaXKWH2Kclhd",
        "outputId": "96273525-72f0-40e6-d600-5ba64ccb4eb7"
      },
      "source": [
        "import torch\n",
        "a = torch.randn(4, 4)\n",
        "a"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1545,  0.9309, -1.6141, -1.4686],\n",
              "        [ 0.3969, -0.4319,  0.2086, -0.6068],\n",
              "        [ 0.2030, -2.0247,  1.2353,  0.2695],\n",
              "        [-0.5049,  0.7329,  1.6494,  2.6579]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvma9HFplQO7",
        "outputId": "04bbd98a-ac2b-4793-ba9a-9cf9a96a27f9"
      },
      "source": [
        "torch.mean(a, 0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0149, -0.1982,  0.3698,  0.2130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    }
  ]
}